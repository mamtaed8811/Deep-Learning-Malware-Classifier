from keras import applications
from keras.preprocessing.image import ImageDataGenerator
from keras import optimizers
from keras.models import Sequential, Model
from keras.layers import Dropout, Dense, Flatten, Input
import numpy as np
from keras.utils import np_utils
#from keras.utils.np_utils import probas_to_classes
import matplotlib.pyplot as plt
#to suppress the warnings for decompression bomb DOS attack
from PIL import Image
Image.MAX_IMAGE_PIXELS = None

#to match the image dimension in tf and the code which is in theano
from keras import backend as K
K.set_image_dim_ordering('th')

#to suppress tf warnings on terminal
import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'
import tensorflow as tf

#path to the model weights files
weights_path = 'D:/Mamta/Project/Data/models/vgg16_weights.h5'
top_model_weights_path = 'D:/Mamta/Project/Data/models/model2/model2_weights.h5'
finetune_weights_path = 'D:/Mamta/Project/Data/models/model3/model3_weights.h5'

#dimensions of our images
img_width, img_height = 150, 150


train_data_dir = 'D:/Mamta/Project/Data/Data_arrangement_families/Train/'
validation_data_dir = 'D:/Mamta/Project/Data/Data_arrangement_families/Validation/'
test_data_dir = 'D:/Mamta/Project/Data/Data_arrangement_families/Prediction_Test/'
nb_train_samples = 8690
nb_test_samples = 1086
nb_validation_samples = 1092
epochs = 50
batch_size = 128

#build the vgg16 network
input_tensor = Input(shape = (3,150,150))
model = applications.VGG16(weights = 'imagenet', include_top = False, input_tensor = input_tensor)
print('Model loaded.')

#build a classifier model on top of the convolution model

top_model = Sequential()
top_model.add(Flatten(input_shape = model.output_shape[1:]))
top_model.add(Dense(256, activation = 'relu'))
top_model.add(Dropout(0.5))
top_model.add(Dense(9, activation = 'softmax'))

#it is necessary to start with a fully-trained classifier, including the top classifier,
#in order to successfully do fine-tuning
top_model.load_weights(top_model_weights_path)

#add the model on top of the vgg16-convolutional network
#model.add(top_model)
model = Model(inputs = model.input, outputs = top_model(model.output))
#set the first 18 layers(up to the last convolutional block) to non-trainable
#weights will not be updated
for layer in model.layers[:15]:
	layer.trainable = False

#compile the model with a SGD/momentum optimizer
model.compile(loss = 'categorical_crossentropy',
	          optimizer = optimizers.SGD(lr = 1e-4, momentum = 0.9),
	          metrics = ['accuracy'])


#prepare data augmentation configuration
train_datagen = ImageDataGenerator(
	            rescale = 1./255,
	            shear_range = 0.2,
	            zoom_range = 0.2,
	            horizontal_flip = True)

validation_datagen = ImageDataGenerator(rescale = 1./255)

test_datagen = ImageDataGenerator(rescale = 1./255)

train_generator = train_datagen.flow_from_directory(
	               train_data_dir,
	               target_size = (img_width, img_height),
	               batch_size = batch_size,
	               class_mode = 'categorical')
model3_features_train = model.predict_generator(
	                       train_generator, nb_train_samples / batch_size)
np.save(open('model3_features_train','wb'), model3_features_train)


validation_generator = validation_datagen.flow_from_directory(
	                   validation_data_dir,
	                   target_size = (img_width, img_height),
	               	   batch_size = batch_size,
	                   class_mode = 'categorical')

model3_features_validation = model.predict_generator(
	                       validation_generator, nb_validation_samples / batch_size)
np.save(open('model3_features_validation','wb'), model3_features_validation)

test_generator = test_datagen.flow_from_directory(
		        test_data_dir,
		        target_size = (img_width, img_height),
		        batch_size = batch_size,
		        class_mode = 'categorical')

model3_features_test = model.predict_generator(
	                       test_generator, nb_test_samples / batch_size)
np.save(open('model3_features_test','wb'), model3_features_test)


#fine-tune the model
history = model.fit_generator(
	    train_generator,
	    steps_per_epoch = nb_train_samples / batch_size,
	    epochs = epochs,
	    validation_data = validation_generator,
	    validation_steps = nb_validation_samples / batch_size)


#saving the model and the weights
# model3_json = model.to_json()
# with open("model3.json","w") as json_file:
# 	json_file.write(model3_json)
	
# # # #serialize weights to HDF5
# model.save_weights(finetune_weights_path)
# model.save('D:/Mamta/Project/Data/models/model3/model3.h5')
# print("Saved model to disk")
# print(model.summary())

# summarize history for accuracy
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()


# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()


