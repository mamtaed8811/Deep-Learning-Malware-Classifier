from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras import backend as K 
import h5py
import matplotlib.pyplot as plt
import numpy as np 

from keras import callbacks
from keras.callbacks import ModelCheckpoint, TensorBoard, Callback
from keras.utils import np_utils
from keras.models import load_model
from keras.preprocessing.image import img_to_array, load_img

#to suppress the warnings for decompression bomb DOS attack
from PIL import Image
Image.MAX_IMAGE_PIXELS = None

import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'
import tensorflow as tf

# dimensions of our images
img_width, img_height = 150, 150

train_data_dir = 'D:/Mamta/Project/Data/Data_arrangement_families/Train/'
validation_data_dir = 'D:/Mamta/Project/Data/Data_arrangement_families/Validation/'
test_data_dir = 'D:/Mamta/Project/Data/Data_arrangement_families/Prediction_Test/'
nb_train_samples = 8690
nb_validation_samples = 1092
nb_test_samples = 1086
epochs = 50
batch_size = 256

if K.image_data_format() == 'channels_first':
	input_shape = (1, img_width, img_height)
else:
	input_shape = (img_width, img_height, 1)

model = Sequential()
model.add(Conv2D(32, (3,3), input_shape = input_shape))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size = (2,2)))

model.add(Conv2D(32, (3,3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size = (2,2)))

model.add(Conv2D(64, (3,3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size = (2,2)))

model.add(Flatten()) #convert 3d vector to 1d vector
model.add(Dense(64))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(9))
model.add(Activation('softmax'))

model.compile(loss = 'categorical_crossentropy',
	          optimizer = 'adam',
	           metrics = ['accuracy'])

# this is the augmentation configuration we will use for training
train_datagen = ImageDataGenerator(
	            rescale = 1. / 255,
	            shear_range = 0.2,
	            zoom_range = 0.2,
	            horizontal_flip = True)

# this is the augmentation configuration we will use for testing:
# only rescaling
validation_datagen = ImageDataGenerator(rescale = 1. / 255)

test_datagen = ImageDataGenerator(rescale = 1. / 255)

train_generator = train_datagen.flow_from_directory(
	             train_data_dir,
	             color_mode='grayscale',
	             target_size = (img_width, img_height),
	              batch_size = batch_size,
	              class_mode = 'categorical')

model1_features_train = model.predict_generator(
		                   train_generator, nb_train_samples / batch_size)

np.save(open('model1_features_train','wb'), model1_features_train)

validation_generator = validation_datagen.flow_from_directory(
	             validation_data_dir,
	             color_mode='grayscale',
	              target_size = (img_width, img_height),
	              batch_size = batch_size,
	              class_mode = 'categorical')

model1_features_validation = model.predict_generator(
		                   validation_generator, nb_validation_samples / batch_size)

np.save(open('model1_features_validation','wb'), model1_features_validation)


# # generating the testing data
test_generator = test_datagen.flow_from_directory(
                  test_data_dir,
                  color_mode='grayscale',
                  target_size=(img_width, img_height),
    			  batch_size=batch_size,
 				  class_mode='categorical')

model1_features_test = model.predict_generator(
		                   test_generator, nb_test_samples / batch_size)

np.save(open('model1_features_test','wb'), model1_features_test)


train_data = np.load(open('model1_features_train', "rb"))
train_labels = np.array([0]*1232 + [1]*1982 + [2]*2353 + [3]*380 + [4]*33 + [5]*600 + [6]*318 + [7]*982 + [8]*810 )
train_labels = np_utils.to_categorical(train_labels)

validation_data = np.load(open('model1_features_validation', "rb"))
validation_labels = np.array([0]*155 + [1]*248 + [2]*295 + [3]*48 + [4]*5 + [5]*76 + [6]*40 + [7]*123 + [8]*102)
validation_labels = np_utils.to_categorical(validation_labels)

test_data = np.load(open('model1_features_test', "rb"))
test_labels= np.array([0]*154 + [1]*248 + [2]*294 + [3]*47 + [4]*4 + [5]*75 + [6]*40 + [7]*123 + [8]*101)
test_labels = np_utils.to_categorical(test_labels)

#history = 

model.fit_generator(
	   						train_generator,
	   						steps_per_epoch = nb_train_samples / batch_size,
	   						epochs = epochs,
	   						validation_data = validation_generator,
	   						validation_steps = nb_validation_samples / batch_size
	   						#callbacks=[checkpointer, tensorboard, TestCallback((test_data, test_labels))]
	    					)


#saving model and weights
# model1_json = model.to_json()
# with open("model1.json","w") as json_file:
# 	json_file.write(model1_json)
	
# #serialize weights to HDF5
# model.save_weights('D:/Mamta/Project/Data/models/model1/model1_weights.h5')
# model.save('D:/Mamta/Project/Data/models/model1/model1.h5')
# print("Saved model to disk")
# print(model.summary())

# def test_the_model():

	
# 	model = load_model('D:/Mamta/Project/Data/models/model1/model1.h5')
# 	model.compile(optimizer='adam',
# 		          loss='categorical_crossentropy', 
# 		          metrics=['accuracy'])
	
# 	features_test_ev = model.evaluate(test_data, test_labels, verbose=0)
# 	print(features_test_ev)

#test_the_model()

# # list all data in history
# print(history.history.keys())

# # summarize history for accuracy
# plt.plot(history.history['acc'])
# plt.plot(history.history['val_acc'])
# plt.title('model accuracy')
# plt.ylabel('accuracy')
# plt.xlabel('epoch')
# plt.legend(['train', 'test'], loc='upper left')
# plt.show()


# # # summarize history for loss
# plt.plot(history.history['loss'])
# plt.plot(history.history['val_loss'])
# plt.title('model loss')
# plt.ylabel('loss')
# plt.xlabel('epoch')
# plt.legend(['train', 'test'], loc='upper left')
# plt.show()


#at epoch 49, val_accuracy is 97.53 so best is to stop at 49th epoch