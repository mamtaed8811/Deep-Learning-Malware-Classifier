import numpy as np 
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Dropout, Dense, Flatten
from keras import applications
from keras.utils import np_utils
from keras.models import model_from_json
import h5py
import matplotlib.pyplot as plt
# #to suppress the warnings for decompression bomb DOS attack
# from PIL import Image
# Image.MAX_IMAGE_PIXELS = None

from keras.models import load_model
from keras.preprocessing.image import img_to_array, load_img

from keras import backend as K
K.set_image_dim_ordering('tf')

import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'
import tensorflow as tf

# dimensions of our images.
img_width, img_height = 150, 150

top_model_weights_path = 'C:/Mamta/Project/Data/models/model2/model2_weights.h5'
train_data_dir = 'C:/Mamta/Project/Data/Data_arrangement_families/Train/'
validation_data_dir = 'C:/Mamta/Project/Data/Data_arrangement_families/Validation/'
test_data_dir = 'C:/Mamta/Project/Data/Data_arrangement_families/Prediction_Test/'
nb_train_samples = 8690
nb_validation_samples = 1092
nb_test_samples = 1086
epochs = 50
batch_size = 128


def save_bottleneck_features():
	datagen = ImageDataGenerator(rescale = 1./255)

	#build the VGG16 network

	model = applications.VGG16(include_top = False, weights = 'imagenet')

	generator = datagen.flow_from_directory(
		       train_data_dir,
		       target_size = (img_width, img_height),
		       batch_size = batch_size,
		       class_mode = None,
		       shuffle = False)

	model2_features_train = model.predict_generator(
		                   generator, nb_train_samples / batch_size)

	np.save(open('model2_features_train','wb'), model2_features_train)

	generator = datagen.flow_from_directory(
		        validation_data_dir,
		        target_size = (img_width, img_height),
		        batch_size = batch_size,
		        class_mode = None,
		        shuffle = False)

	model2_features_validation = model.predict_generator(
		                       generator, nb_validation_samples / batch_size)
	np.save(open('model2_features_validation','wb'), model2_features_validation)

	generator = datagen.flow_from_directory(
		        test_data_dir,
		        target_size = (img_width, img_height),
		        batch_size = batch_size,
		        class_mode = None,
		        shuffle = False)

	model2_features_test = model.predict_generator(
		                       generator, nb_test_samples / batch_size)
	np.save(open('model2_features_test','wb'), model2_features_test)



def train_top_model():
	train_data = np.load(open('model2_features_train','rb'))
	train_labels = np.array(
		[0]*1232 + [1]*1982 + [2]*2353 + [3]*380 + [4]*33 + [5]*600 + [6]*318 + [7]*982 + [8]*810 )
	train_labels = np_utils.to_categorical(train_labels)

	validation_data = np.load(open('model2_features_validation','rb'))
	validation_labels = np.array(
		[0]*155 + [1]*248 + [2]*295 + [3]*48 + [4]*5 + [5]*76 + [6]*40 + [7]*123 + [8]*102)
	validation_labels = np_utils.to_categorical(validation_labels)


	model = Sequential()
	model.add(Flatten(input_shape= train_data.shape[1:]))
	model.add(Dense(256, activation = 'relu'))
	model.add(Dropout(0.5))
	model.add(Dense(9, activation = 'softmax'))

	model.compile(optimizer = 'adam',
		          loss = 'categorical_crossentropy',
		          metrics = ['accuracy'])

	history = model.fit(train_data, train_labels,
		      epochs = epochs,
		      batch_size = batch_size,
		      validation_data = (validation_data, validation_labels))

	model2_json = model.to_json()
	with open("model2.json","w") as json_file:
		json_file.write(model2_json)

	# #serialize weights to HDF5
	model.save_weights(top_model_weights_path)
	model.save('C:/Mamta/Project/Data/models/model2/model2.h5')
	print("Saved model to disk")

	print(model.summary())


def predict_image_class(file):
    model = applications.VGG16(include_top=False, weights='imagenet')
    x = load_img(file, target_size=(img_width,img_height))
    x = img_to_array(x)
    x = np.expand_dims(x, axis=0)
    array = model.predict(x)
    model = Sequential()
    model.add(Flatten(input_shape=array.shape[1:]))
    model.add(Dense(256, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(9, activation='softmax'))
    model.load_weights(top_model_weights_path)
    class_predicted = model.predict_classes(array)
    if class_predicted==0:
        print("Ramnit")
    elif class_predicted==1:
        print("Lollipop") 
    elif class_predicted==2:
        print("Kelihos_ver3") 
    elif class_predicted==3:
        print("Vundo")
    elif class_predicted==4:
        print("Simda") 
    elif class_predicted==5:
        print("Tracur") 
    elif class_predicted==6:
        print("Kelihos_ver1") 
    elif class_predicted==7:
        print("Obfuscator.ACY")
    elif class_predicted==8:
        print("Gatak")      


#for training

save_bottleneck_features()

train_top_model()

predict_image_class("C:/Mamta/Project/Data/Data_arrangement_families/Prediction_Test_unclassified/3zZpqyclD9B2v5Qas18m.jpg")


# # summarize history for accuracy
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()


# # summarize history for loss

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
